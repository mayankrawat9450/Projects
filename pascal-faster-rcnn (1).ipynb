{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nfrom torch.utils.data import DataLoader\nimport torchvision\nfrom torchvision.models.detection import FasterRCNN\nfrom torchvision.models.detection.rpn import AnchorGenerator\nfrom torchvision.transforms import functional as F\nfrom torchvision.datasets import VOCDetection\nimport torchvision.transforms as T\nfrom tqdm import tqdm\nimport numpy as np\nimport os\nimport torch.nn as nn\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-06-02T13:11:50.934850Z","iopub.execute_input":"2025-06-02T13:11:50.935096Z","iopub.status.idle":"2025-06-02T13:11:50.939678Z","shell.execute_reply.started":"2025-06-02T13:11:50.935072Z","shell.execute_reply":"2025-06-02T13:11:50.938747Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"\nclass Bottleneck(nn.Module):\n    expansion = 4\n    def __init__(self, in_channels, out_channels, stride=1, downsample=None):\n        super(Bottleneck, self).__init__()\n        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(out_channels)\n        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(out_channels)\n        self.conv3 = nn.Conv2d(out_channels, out_channels * self.expansion, kernel_size=1, stride=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(out_channels * self.expansion)\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n\n    def forward(self, x):\n        identity = x\n        out = self.relu(self.bn1(self.conv1(x)))\n        out = self.relu(self.bn2(self.conv2(out)))\n        out = self.bn3(self.conv3(out))\n        if self.downsample is not None:\n            identity = self.downsample(x)\n        out += identity\n        return self.relu(out)\n\nclass ResNet50(nn.Module):\n    def __init__(self, block=Bottleneck, layers=[3, 4, 6, 3], num_classes=1000):\n        super(ResNet50, self).__init__()\n        self.in_channels = 64\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n        self.bn1 = nn.BatchNorm2d(64)\n        self.relu = nn.ReLU(inplace=True)\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        self.layer1 = self._make_layer(block, 64, layers[0], stride=1)\n        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n        self.fc = nn.Linear(512 * block.expansion, num_classes)\n\n    def _make_layer(self, block, out_channels, blocks, stride=1):\n        downsample = None\n        if stride != 1 or self.in_channels != out_channels * block.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2d(self.in_channels, out_channels * block.expansion, kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(out_channels * block.expansion),\n            )\n        layers = [block(self.in_channels, out_channels, stride, downsample)]\n        self.in_channels = out_channels * block.expansion\n        for _ in range(1, blocks):\n            layers.append(block(self.in_channels, out_channels))\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        x = self.relu(self.bn1(self.conv1(x)))\n        x = self.maxpool(x)\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n        x = self.avgpool(x)\n        x = torch.flatten(x, 1)\n        x = self.fc(x)\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-02T13:11:58.207348Z","iopub.execute_input":"2025-06-02T13:11:58.208047Z","iopub.status.idle":"2025-06-02T13:11:58.218931Z","shell.execute_reply.started":"2025-06-02T13:11:58.208021Z","shell.execute_reply":"2025-06-02T13:11:58.218080Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"from torchvision.models.detection import FasterRCNN\nfrom torchvision.models.detection.rpn import AnchorGenerator\nfrom torchvision.ops import MultiScaleRoIAlign\n\nclass ResNetBackbone(nn.Module):\n    def __init__(self):\n        super().__init__()\n        resnet = ResNet50()\n        self.body = nn.Sequential(\n            resnet.conv1,\n            resnet.bn1,\n            resnet.relu,\n            resnet.maxpool,\n            resnet.layer1,\n            resnet.layer2,\n            resnet.layer3,\n            resnet.layer4\n        )\n        self.out_channels = 2048\n\n    def forward(self, x):\n        return self.body(x)\n\ndef get_model(num_classes=21):  # 20 VOC classes + background\n    backbone = ResNetBackbone()\n    backbone.out_channels = 2048\n\n    rpn_anchor_generator = AnchorGenerator(\n        sizes=((32, 64, 128, 256, 512),),\n        aspect_ratios=((0.5, 1.0, 2.0),)\n    )\n\n    roi_pooler = MultiScaleRoIAlign(\n        featmap_names=[\"0\"],\n        output_size=7,\n        sampling_ratio=2\n    )\n\n    model = FasterRCNN(\n        backbone,\n        num_classes=num_classes,\n        rpn_anchor_generator=rpn_anchor_generator,\n        box_roi_pool=roi_pooler\n    )\n\n    return model\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-02T13:11:58.971822Z","iopub.execute_input":"2025-06-02T13:11:58.972064Z","iopub.status.idle":"2025-06-02T13:11:58.978616Z","shell.execute_reply.started":"2025-06-02T13:11:58.972048Z","shell.execute_reply":"2025-06-02T13:11:58.977741Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"from torchvision.datasets import VOCDetection\ndataset = VOCDetection(root=\"./\", year=\"2007\", image_set=\"train\", download=True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-02T13:12:00.146394Z","iopub.execute_input":"2025-06-02T13:12:00.147120Z","iopub.status.idle":"2025-06-02T13:12:20.464517Z","shell.execute_reply.started":"2025-06-02T13:12:00.147094Z","shell.execute_reply":"2025-06-02T13:12:20.463994Z"}},"outputs":[{"name":"stderr","text":"100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 460M/460M [00:16<00:00, 27.5MB/s] \n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"import os\n\nprint(\"Folders inside ./VOCdevkit:\")\nprint(os.listdir(\"./VOCdevkit\"))\n\nprint(\"Folders inside ./VOCdevkit/VOC2007:\")\nprint(os.listdir(\"./VOCdevkit/VOC2007\"))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-02T13:12:22.477049Z","iopub.execute_input":"2025-06-02T13:12:22.477317Z","iopub.status.idle":"2025-06-02T13:12:22.482467Z","shell.execute_reply.started":"2025-06-02T13:12:22.477297Z","shell.execute_reply":"2025-06-02T13:12:22.481734Z"}},"outputs":[{"name":"stdout","text":"Folders inside ./VOCdevkit:\n['VOC2007']\nFolders inside ./VOCdevkit/VOC2007:\n['SegmentationObject', 'SegmentationClass', 'JPEGImages', 'Annotations', 'ImageSets']\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"import os\nimport urllib.request\nimport tarfile\nfrom PIL import Image\nimport torch\nfrom torch.utils.data import DataLoader\nimport torchvision.transforms.functional as F\nimport xml.etree.ElementTree as ET\n\n# 1. Download + Extract VOC2007 if not done already\n\ndata_dir = \"./VOCdevkit\"\nvoc_tar_path = \"./VOCtrainval_06-Nov-2007.tar\"\nVOC_URL = \"http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtrainval_06-Nov-2007.tar\"\n\nif not os.path.exists(voc_tar_path):\n    print(\"Downloading VOC2007 dataset...\")\n    urllib.request.urlretrieve(VOC_URL, voc_tar_path)\n    print(\"Download complete.\")\n\n# Extract dataset if not extracted\nif not os.path.exists(os.path.join(data_dir, \"VOCdevkit\", \"VOC2007\")):\n    print(\"Extracting VOC2007 dataset...\")\n    with tarfile.open(voc_tar_path) as tar:\n        tar.extractall(path=data_dir)  # this creates VOCdevkit/VOCdevkit/VOC2007\n    print(\"Extraction complete.\")\n\n# 2. Define dataset class\n\nVOC_CLASSES = [\n    \"__background__\",\n    \"aeroplane\", \"bicycle\", \"bird\", \"boat\",\n    \"bottle\", \"bus\", \"car\", \"cat\", \"chair\",\n    \"cow\", \"diningtable\", \"dog\", \"horse\",\n    \"motorbike\", \"person\", \"pottedplant\",\n    \"sheep\", \"sofa\", \"train\", \"tvmonitor\"\n]\nclass_name_to_idx = {cls_name: idx for idx, cls_name in enumerate(VOC_CLASSES)}\n\nclass PascalVOCDataset(torch.utils.data.Dataset):\n    def __init__(self, image_dir, annotation_dir, transforms=None):\n        self.image_dir = image_dir\n        self.annotation_dir = annotation_dir\n        self.transforms = transforms\n        self.image_files = [f for f in os.listdir(image_dir) if f.endswith(\".jpg\")]\n        self.image_files.sort()\n\n    def __len__(self):\n        return len(self.image_files)\n\n    def __getitem__(self, idx):\n        img_name = self.image_files[idx]\n        img_path = os.path.join(self.image_dir, img_name)\n        image = Image.open(img_path).convert(\"RGB\")\n        image = F.to_tensor(image)\n\n        ann_path = os.path.join(self.annotation_dir, img_name.replace(\".jpg\", \".xml\"))\n        boxes = []\n        labels = []\n\n        tree = ET.parse(ann_path)\n        root = tree.getroot()\n        for obj in root.findall(\"object\"):\n            cls_name = obj.find(\"name\").text.lower().strip()\n            if cls_name not in class_name_to_idx:\n                continue\n            label = class_name_to_idx[cls_name]\n            labels.append(label)\n\n            bbox = obj.find(\"bndbox\")\n            xmin = float(bbox.find(\"xmin\").text)\n            ymin = float(bbox.find(\"ymin\").text)\n            xmax = float(bbox.find(\"xmax\").text)\n            ymax = float(bbox.find(\"ymax\").text)\n            boxes.append([xmin, ymin, xmax, ymax])\n\n        boxes = torch.tensor(boxes, dtype=torch.float32)\n        labels = torch.tensor(labels, dtype=torch.int64)\n\n        target = {\"boxes\": boxes, \"labels\": labels, \"image_id\": torch.tensor([idx])}\n\n        if self.transforms:\n            image, target = self.transforms(image, target)\n\n        return image, target\n\n# 3. Update paths to match your folder structure:\n\nimage_dir = \"./VOCdevkit/VOCdevkit/VOC2007/JPEGImages\"\nannotation_dir = \"./VOCdevkit/VOCdevkit/VOC2007/Annotations\"\n\ndataset = PascalVOCDataset(image_dir, annotation_dir)\n\ndef collate_fn(batch):\n    return tuple(zip(*batch))\n\ndata_loader = DataLoader(dataset, batch_size=2, shuffle=True, collate_fn=collate_fn)\n\n# 4. Test by loading one batch\n\nimages, targets = next(iter(data_loader))\nprint(f\"Loaded batch with {len(images)} images\")\nprint(f\"Image[0] shape: {images[0].shape}\")\nprint(f\"Target[0] keys: {targets[0].keys()}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-02T13:12:23.721876Z","iopub.execute_input":"2025-06-02T13:12:23.722329Z","iopub.status.idle":"2025-06-02T13:12:25.965896Z","shell.execute_reply.started":"2025-06-02T13:12:23.722305Z","shell.execute_reply":"2025-06-02T13:12:25.965082Z"}},"outputs":[{"name":"stdout","text":"Extracting VOC2007 dataset...\nExtraction complete.\nLoaded batch with 2 images\nImage[0] shape: torch.Size([3, 332, 500])\nTarget[0] keys: dict_keys(['boxes', 'labels', 'image_id'])\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"from torchvision.ops import box_iou\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-02T13:12:30.580288Z","iopub.execute_input":"2025-06-02T13:12:30.580778Z","iopub.status.idle":"2025-06-02T13:12:30.585319Z","shell.execute_reply.started":"2025-06-02T13:12:30.580748Z","shell.execute_reply":"2025-06-02T13:12:30.584571Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"def evaluate_model(model, data_loader, device, iou_threshold=0.5, max_batches=5):\n    model.eval()\n    correct = 0\n    total = 0\n\n    with torch.no_grad():\n        for i, (images, targets) in enumerate(data_loader):\n            if i >= max_batches:\n                break\n            images = [img.to(device) for img in images]\n            outputs = model(images)\n\n            for output, target in zip(outputs, targets):\n                pred_boxes = output[\"boxes\"].cpu()\n                gt_boxes = target[\"boxes\"].cpu()\n\n                if len(pred_boxes) == 0 or len(gt_boxes) == 0:\n                    continue\n\n                ious = box_iou(pred_boxes, gt_boxes)\n                matched = (ious >= iou_threshold).any(dim=1)\n                correct += matched.sum().item()\n                total += len(gt_boxes)\n\n    accuracy = 100.0 * correct / total if total > 0 else 0.0\n    return accuracy\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-02T13:12:32.291884Z","iopub.execute_input":"2025-06-02T13:12:32.292386Z","iopub.status.idle":"2025-06-02T13:12:32.298233Z","shell.execute_reply.started":"2025-06-02T13:12:32.292366Z","shell.execute_reply":"2025-06-02T13:12:32.297411Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"from tqdm import tqdm\nimport torch\nimport torchvision\n\ndevice = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n\nnum_classes = 21  # VOC classes (20 + background)\nmodel = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\nin_features = model.roi_heads.box_predictor.cls_score.in_features\nmodel.roi_heads.box_predictor = torchvision.models.detection.faster_rcnn.FastRCNNPredictor(in_features, num_classes)\nmodel.to(device)\n\nparams = [p for p in model.parameters() if p.requires_grad]\noptimizer = torch.optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\nlr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n\nnum_epochs = 5\n\nfor epoch in range(num_epochs):\n    model.train()\n    running_loss = 0.0\n\n    loop = tqdm(data_loader, desc=f\"Epoch [{epoch + 1}/{num_epochs}]\", unit=\"batch\")\n\n    for images, targets in loop:\n        images = list(img.to(device) for img in images)\n        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n\n        loss_dict = model(images, targets)\n        losses = sum(loss for loss in loss_dict.values())\n\n        optimizer.zero_grad()\n        losses.backward()\n        optimizer.step()\n\n        running_loss += losses.item()\n        avg_loss = running_loss / (loop.n + 1)\n        loop.set_postfix(loss=avg_loss)\n\n    lr_scheduler.step\n    \n    # Evaluate training accuracy (IoU â‰¥ 0.5)\n    acc = evaluate_model(model, data_loader, device)\n    print(f\"ğŸ“Š Epoch {epoch+1}: Training Accuracy (IoU â‰¥ 0.5) = {acc:.2f}%\\n\")\n\n\nprint(\"Training complete.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-02T13:12:36.521827Z","iopub.execute_input":"2025-06-02T13:12:36.522477Z","iopub.status.idle":"2025-06-02T15:22:56.939945Z","shell.execute_reply.started":"2025-06-02T13:12:36.522456Z","shell.execute_reply":"2025-06-02T15:22:56.939042Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=FasterRCNN_ResNet50_FPN_Weights.COCO_V1`. You can also use `weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\" to /root/.cache/torch/hub/checkpoints/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 160M/160M [00:00<00:00, 198MB/s]  \nEpoch [1/5]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2506/2506 [25:51<00:00,  1.61batch/s, loss=0.446]\n","output_type":"stream"},{"name":"stdout","text":"ğŸ“Š Epoch 1: Training Accuracy (IoU â‰¥ 0.5) = 165.12%\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch [2/5]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2506/2506 [26:01<00:00,  1.61batch/s, loss=0.351]\n","output_type":"stream"},{"name":"stdout","text":"ğŸ“Š Epoch 2: Training Accuracy (IoU â‰¥ 0.5) = 247.22%\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch [3/5]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2506/2506 [26:04<00:00,  1.60batch/s, loss=0.323]\n","output_type":"stream"},{"name":"stdout","text":"ğŸ“Š Epoch 3: Training Accuracy (IoU â‰¥ 0.5) = 222.86%\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch [4/5]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2506/2506 [26:08<00:00,  1.60batch/s, loss=0.308]\n","output_type":"stream"},{"name":"stdout","text":"ğŸ“Š Epoch 4: Training Accuracy (IoU â‰¥ 0.5) = 159.09%\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch [5/5]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2506/2506 [26:04<00:00,  1.60batch/s, loss=0.296]\n","output_type":"stream"},{"name":"stdout","text":"ğŸ“Š Epoch 5: Training Accuracy (IoU â‰¥ 0.5) = 215.79%\n\nTraining complete.\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"!wget -q https://raw.githubusercontent.com/pytorch/vision/main/references/detection/engine.py\n!wget -q https://raw.githubusercontent.com/pytorch/vision/main/references/detection/utils.py\n!wget -q https://raw.githubusercontent.com/pytorch/vision/main/references/detection/transforms.py\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-02T16:01:16.533155Z","iopub.execute_input":"2025-06-02T16:01:16.533415Z","iopub.status.idle":"2025-06-02T16:01:17.459638Z","shell.execute_reply.started":"2025-06-02T16:01:16.533395Z","shell.execute_reply":"2025-06-02T16:01:17.458560Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"!wget -q https://raw.githubusercontent.com/pytorch/vision/main/references/detection/engine.py\n!wget -q https://raw.githubusercontent.com/pytorch/vision/main/references/detection/utils.py\n!wget -q https://raw.githubusercontent.com/pytorch/vision/main/references/detection/transforms.py\n!wget -q https://raw.githubusercontent.com/pytorch/vision/main/references/detection/coco_eval.py\n!wget -q https://raw.githubusercontent.com/pytorch/vision/main/references/detection/coco_utils.py\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-02T16:02:05.237567Z","iopub.execute_input":"2025-06-02T16:02:05.238283Z","iopub.status.idle":"2025-06-02T16:02:06.455742Z","shell.execute_reply.started":"2025-06-02T16:02:05.238255Z","shell.execute_reply":"2025-06-02T16:02:06.454748Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"!pip install -q fvcore\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-02T16:02:41.793086Z","iopub.execute_input":"2025-06-02T16:02:41.793358Z","iopub.status.idle":"2025-06-02T16:02:49.765050Z","shell.execute_reply.started":"2025-06-02T16:02:41.793338Z","shell.execute_reply":"2025-06-02T16:02:49.764255Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m50.2/50.2 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Building wheel for fvcore (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Building wheel for iopath (setup.py) ... \u001b[?25l\u001b[?25hdone\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport matplotlib.patches as patches\nfrom torchvision.ops import nms\n\n# Load label map\nidx_to_class = {v: k for k, v in class_name_to_idx.items()}\n\n# Function to visualize predictions\ndef visualize_prediction(image, boxes, labels, scores, threshold=0.5):\n    image = image.permute(1, 2, 0).cpu().numpy()\n    fig, ax = plt.subplots(1, figsize=(12, 8))\n    ax.imshow(image)\n\n    for box, label, score in zip(boxes, labels, scores):\n        if score < threshold:\n            continue\n        xmin, ymin, xmax, ymax = box\n        rect = patches.Rectangle(\n            (xmin, ymin), xmax - xmin, ymax - ymin,\n            linewidth=2, edgecolor='red', facecolor='none'\n        )\n        ax.add_patch(rect)\n        ax.text(xmin, ymin - 10, f\"{idx_to_class[label]}: {score:.2f}\", \n                color='white', fontsize=12, backgroundcolor='red')\n\n    plt.axis('off')\n    plt.show()\n\n# Load model\nmodel.eval()\n\n# Test on a few samples from the test loader\nnum_test_images = 3\nfor i, (images, targets) in enumerate(data_loader):\n    if i >= num_test_images:\n        break\n\n    images = [img.to(device) for img in images]\n\n    with torch.no_grad():\n        outputs = model(images)\n\n    for img, output in zip(images, outputs):\n        boxes = output['boxes'].cpu()\n        labels = output['labels'].cpu()\n        scores = output['scores'].cpu()\n\n        # Optional: apply Non-Maximum Suppression (NMS)\n        keep = nms(boxes, scores, iou_threshold=0.5)\n        boxes = boxes[keep]\n        labels = labels[keep]\n        scores = scores[keep]\n        print(\"Number of boxes:\", len(boxes))\n        print(\"Scores:\", scores)\n        print(\"Detected boxes:\", boxes.shape[0])\n        print(\"Scores:\", scores[:10])  # Print top 10 scores\n\n\n\n        visualize_prediction(img.cpu(), boxes, labels, scores, threshold=0.2)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-02T16:39:22.263627Z","iopub.status.idle":"2025-06-02T16:39:22.263899Z","shell.execute_reply.started":"2025-06-02T16:39:22.263765Z","shell.execute_reply":"2025-06-02T16:39:22.263777Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Ensure model is in evaluation mode\nmodel.eval()\n\n# Evaluate and print test accuracy\ntest_accuracy = evaluate_model(model, data_loader, device, iou_threshold=0.5, max_batches=10)  # You can increase max_batches for more coverage\nprint(f\"âœ… Test Accuracy (IoU â‰¥ 0.5): {test_accuracy:.2f}%\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-02T16:22:20.462035Z","iopub.execute_input":"2025-06-02T16:22:20.462883Z","iopub.status.idle":"2025-06-02T16:22:23.221185Z","shell.execute_reply.started":"2025-06-02T16:22:20.462848Z","shell.execute_reply":"2025-06-02T16:22:23.220524Z"}},"outputs":[{"name":"stdout","text":"âœ… Test Accuracy (IoU â‰¥ 0.5): 161.02%\n","output_type":"stream"}],"execution_count":35},{"cell_type":"code","source":"pip install torchmetrics\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-02T16:32:11.468146Z","iopub.execute_input":"2025-06-02T16:32:11.468820Z","iopub.status.idle":"2025-06-02T16:33:44.066308Z","shell.execute_reply.started":"2025-06-02T16:32:11.468795Z","shell.execute_reply":"2025-06-02T16:33:44.065382Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: torchmetrics in /usr/local/lib/python3.11/dist-packages (1.7.1)\nRequirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.11/dist-packages (from torchmetrics) (1.26.4)\nRequirement already satisfied: packaging>17.1 in /usr/local/lib/python3.11/dist-packages (from torchmetrics) (25.0)\nRequirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from torchmetrics) (2.6.0+cu124)\nRequirement already satisfied: lightning-utilities>=0.8.0 in /usr/local/lib/python3.11/dist-packages (from torchmetrics) (0.14.3)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (75.2.0)\nRequirement already satisfied: typing_extensions in /usr/local/lib/python3.11/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.13.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>1.20.0->torchmetrics) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>1.20.0->torchmetrics) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>1.20.0->torchmetrics) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>1.20.0->torchmetrics) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>1.20.0->torchmetrics) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>1.20.0->torchmetrics) (2.4.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (3.18.0)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (2025.3.2)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (12.4.127)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=2.0.0->torchmetrics)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch>=2.0.0->torchmetrics)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch>=2.0.0->torchmetrics)\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch>=2.0.0->torchmetrics)\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=2.0.0->torchmetrics)\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=2.0.0->torchmetrics)\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=2.0.0->torchmetrics)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->torchmetrics) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->torchmetrics) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>1.20.0->torchmetrics) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>1.20.0->torchmetrics) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>1.20.0->torchmetrics) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>1.20.0->torchmetrics) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>1.20.0->torchmetrics) (2024.2.0)\nDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m27.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.9.41\n    Uninstalling nvidia-nvjitlink-cu12-12.9.41:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.9.41\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.10.19\n    Uninstalling nvidia-curand-cu12-10.3.10.19:\n      Successfully uninstalled nvidia-curand-cu12-10.3.10.19\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.4.0.6\n    Uninstalling nvidia-cufft-cu12-11.4.0.6:\n      Successfully uninstalled nvidia-cufft-cu12-11.4.0.6\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.9.0.13\n    Uninstalling nvidia-cublas-cu12-12.9.0.13:\n      Successfully uninstalled nvidia-cublas-cu12-12.9.0.13\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.9.5\n    Uninstalling nvidia-cusparse-cu12-12.5.9.5:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.9.5\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.7.4.40\n    Uninstalling nvidia-cusolver-cu12-11.7.4.40:\n      Successfully uninstalled nvidia-cusolver-cu12-11.7.4.40\nSuccessfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":39},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torchmetrics.detection.mean_ap import MeanAveragePrecision\n\n# Initialize metric\nmetric = MeanAveragePrecision(iou_type=\"bbox\", iou_thresholds=[0.5])\n\n# Collect predictions and ground truths\nmodel.eval()\nwith torch.no_grad():\n    for images, targets in data_loader:\n        images = [img.to(device) for img in images]\n        outputs = model(images)\n\n        # Convert to required format\n        preds = []\n        gts = []\n        for output, target in zip(outputs, targets):\n            preds.append({\n                \"boxes\": output[\"boxes\"].cpu(),\n                \"scores\": output[\"scores\"].cpu(),\n                \"labels\": output[\"labels\"].cpu()\n            })\n            gts.append({\n                \"boxes\": target[\"boxes\"].cpu(),\n                \"labels\": target[\"labels\"].cpu()\n            })\n\n        metric.update(preds, gts)\n\n# Compute mAP\nresults = metric.compute()\nprint(f\"ğŸ“Š mAP@0.5: {results['map_50']:.4f}\")\nprint(f\"ğŸ“Š mAP@[.5:.95]: {results['map']:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-02T16:46:38.453834Z","iopub.execute_input":"2025-06-02T16:46:38.454191Z","iopub.status.idle":"2025-06-02T16:59:07.060987Z","shell.execute_reply.started":"2025-06-02T16:46:38.454159Z","shell.execute_reply":"2025-06-02T16:59:07.059978Z"}},"outputs":[{"name":"stdout","text":"ğŸ“Š mAP@0.5: 0.0002\nğŸ“Š mAP@[.5:.95]: 0.0002\n","output_type":"stream"}],"execution_count":46},{"cell_type":"code","source":"from torchvision.models.detection import fasterrcnn_resnet50_fpn\nfrom fvcore.nn import FlopCountAnalysis, parameter_count\nimport torch\n\n# Dummy input\nmodel.eval()\ndummy_input = [torch.randn(3, 300, 300).to(device)]\n\n# FLOP analysis\nflops = FlopCountAnalysis(model, dummy_input)\nparams = parameter_count(model)\n\nprint(f\"ğŸ§® Total FLOPs: {flops.total() / 1e9:.2f} GFLOPs\")\nprint(f\"ğŸ“¦ Total Parameters: {params[''] / 1e6:.2f} Million\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-02T16:59:16.113460Z","iopub.execute_input":"2025-06-02T16:59:16.113739Z","iopub.status.idle":"2025-06-02T16:59:17.507619Z","shell.execute_reply.started":"2025-06-02T16:59:16.113719Z","shell.execute_reply":"2025-06-02T16:59:17.506844Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/functional.py:4624: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  * torch.tensor(scale_factors[i], dtype=torch.float32)\n/usr/local/lib/python3.11/dist-packages/torchvision/ops/boxes.py:166: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  boxes_x = torch.min(boxes_x, torch.tensor(width, dtype=boxes.dtype, device=boxes.device))\n/usr/local/lib/python3.11/dist-packages/torchvision/ops/boxes.py:168: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  boxes_y = torch.min(boxes_y, torch.tensor(height, dtype=boxes.dtype, device=boxes.device))\n/usr/local/lib/python3.11/dist-packages/torchvision/models/detection/transform.py:308: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  torch.tensor(s, dtype=torch.float32, device=boxes.device)\n/usr/local/lib/python3.11/dist-packages/torchvision/models/detection/transform.py:309: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  / torch.tensor(s_orig, dtype=torch.float32, device=boxes.device)\n","output_type":"stream"},{"name":"stdout","text":"ğŸ§® Total FLOPs: 134.04 GFLOPs\nğŸ“¦ Total Parameters: 41.40 Million\n","output_type":"stream"}],"execution_count":47},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}